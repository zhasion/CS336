import torch
import transformers
import logging
import sys
from jaxtyping import Int, Float
from vllm.model_executor import set_random_seed as vllm_set_random_seed
from vllm import LLM, SamplingParams
from unittest.mock import patch

def tokenize_prompt_and_output(
    prompt_strs: list[str], 
    output_strs: list[str], 
    tokenizer: transformers.PreTrainedTokenizer,
    device: str | None = 'cpu'
) -> dict[str, torch.Tensor]:
    
    batch_size = len(prompt_strs)

    prompt_ids = [tokenizer.encode(x) for x in prompt_strs]
    output_ids = [tokenizer.encode(x) for x in output_strs]
        
    max_lens = max([len(x) + len(y) for x, y in zip(prompt_ids, output_ids)])

    input_ids = torch.full((batch_size, max_lens), fill_value=tokenizer.pad_token_id, device=device)
    labels = torch.full((batch_size, max_lens), fill_value=tokenizer.pad_token_id, device=device)
    response_mask = torch.zeros((batch_size, max_lens), dtype=torch.bool, device=device)

    for idx, (prompt_id, output_id) in enumerate(zip(prompt_ids, output_ids)):
        concat_token = torch.tensor(prompt_id + output_id)
        len_prompt = len(prompt_id)
        len_output = len(output_id)
        len_concat = len_prompt + len_output

        input_ids[idx, :len_concat] = concat_token
        labels[idx, :len_concat - 1] = concat_token[1:]
        # response_mask: a boolean mask that is True for all tokens in the response, 
        # and False for all question and padding tokens.
        response_mask[idx, len_prompt-1:len_concat-1] = True

    result = {
        'input_ids' : input_ids[:, :-1],
        'response_mask' : response_mask[:, :-1],
        'labels' : labels[:, :-1]
    }
    return result


def compute_entropy(logits: Float[torch.Tensor, 'bs seq vocab']) -> Float[torch.Tensor, 'bs seq']:
    log_softmax = logits.log_softmax(dim=-1)
    entropy = -torch.sum(log_softmax * log_softmax.exp(), dim=-1, keepdim=False)
    return entropy


def get_response_log_probs(
    model: transformers.PreTrainedModel,
    input_ids: Int[torch.Tensor, 'bs seq'],
    labels: Int[torch.Tensor, 'bs seq'],
    return_token_entropy: bool = False
) -> dict[str, torch.Tensor]:
    logits = model(input_ids).logits
    log_softmax = torch.log_softmax(logits, dim=-1) # [bs, seq, vocab_size]
    log_probs = torch.gather(input=log_softmax, index=labels[..., None], dim=-1).squeeze()
    token_entropy = compute_entropy(logits=logits) if return_token_entropy else None

    return_data = {
        'log_probs': log_probs,
        'token_entropy': token_entropy
    }

    return return_data


def masked_normalize(
    tensor: Float[torch.Tensor, 'bs seq'],
    mask: Float[torch.Tensor, 'bs seq'],
    normalize_constant: float,
    dim: int | None = None
) -> torch.Tensor:
    # masked = tensor * mask
    # return masked.sum(dim) / mask.sum(dim) / normalize_constant
    return torch.sum(tensor.masked_fill(mask==0, 0), dim=dim) / normalize_constant
    

def sft_microbatch_train_step(
    policy_log_probs: Float[torch.Tensor, 'bs seq'],
    response_mask: Int[torch.Tensor, 'bs seq'],
    gradient_accumulation_steps: int,
    normalize_constant: float=1.0
) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
    
    log_prob = masked_normalize(
        tensor=policy_log_probs,
        mask=response_mask,
        normalize_constant=normalize_constant,
        dim=-1
    )

    loss = -log_prob.mean() / gradient_accumulation_steps
    loss.backward()

    metadata = {}

    return (loss, metadata)


def log_generations(
    step: int,
    model: transformers.AutoModelForCausalLM,
    tokenizer: transformers.AutoTokenizer, 
    input_prompt: str,
    complete: str,
):
    # 1. The input prompt.

    # 2. The response generated by the SFT/RL model.

    # 3. The ground-truth answer.

    # 4. The reward information, including format, answer, and total reward.

    # 5. The average token entropy of the response.

    # 6. The average response length, average response length for correct responses, and average response length for incorrect responses.
    pass


def load_policy_into_vllm_instance(
    policy: transformers.PreTrainedModel, 
    llm: LLM
):
    state_dict = policy.state_dict()
    # llm_model = llm.llm_engine.engine_core.driver_worker.model_runner.model
    # llm_model = llm.llm_engine.works[0].model
    # llm_model = llm.model_executor.driver_worker.model_runner.model
    
    llm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model
    llm_model.load_weights(state_dict.items())


def init_vllm(model_id: str, device: str, seed: int, gpu_memory_utilization: float = 0.85):

    """ Start the inference process, here we use vLLM to hold a model 
    on a GPU separate from the policy.
    """

    vllm_set_random_seed(seed)

    # Monkeypatch from TRL:
    # https://github.com/huggingface/trl/blob/ 
    # 22759c820867c8659d00082ba8cf004e963873c1/trl/trainer/grpo_trainer.py 
    # Patch vLLM to make sure we can 
    # (1) place the vLLM model on the desired device (world_size_patch) and 
    # (2) avoid a test that is not designed for our setting (profiling_patch).

    world_size_patch = patch("torch.distributed.get_world_size", return_value=1)

    profiling_patch = patch("vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling", return_value=None)

    with world_size_patch, profiling_patch:
        return LLM(model=model_id, device=device, dtype=torch.bfloat16, enable_prefix_caching=True, gpu_memory_utilization=gpu_memory_utilization)
        return LLM(model=model_id, tensor_parallel_size=1, dtype=torch.bfloat16, enable_prefix_caching=True, gpu_memory_utilization=gpu_memory_utilization)


def gradient_clip(model: torch.nn.Module):
    # grads = [p.grad.data.flatten() for p in model.parameters() if p.grad is not None]
    # l2_norm = torch.concat(grads).norm()

    l2_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    l2_norm = total_norm ** 0.5

    max_l2_norm = 1.0
    if l2_norm > max_l2_norm:
        clip_coef = max_l2_norm / (l2_norm + 1e-6)
        for p in model.parameters():
            if p.grad is None:
                continue
            p.grad.mul_(clip_coef)


def get_logger(log_path: str) -> logging.Logger:

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    console_handler = logging.StreamHandler(sys.stdout)
    console_formatter = logging.Formatter(
        fmt='%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)

    file_hander = logging.FileHandler(log_path, 'a')
    file_formatter = logging.Formatter(
        fmt='%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_hander.setFormatter(file_formatter)

    logger.addHandler(file_hander)
    logger.addHandler(console_handler)

    return logger


def save_checkpoints(model, tokenizer, output_dir):
    # Save the model weights 
    model.save_pretrained(save_directory=output_dir) 
    tokenizer.save_pretrained(save_directory=output_dir)

    print(model.generate(torch.tensor([tokenizer.encode('who are you')])))
